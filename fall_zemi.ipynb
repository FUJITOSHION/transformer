{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fall_zemi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNzpOmDLvrEuZ9HKPnocoP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FUJITOSHION/transformer/blob/master/fall_zemi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD9O0JkXNA95"
      },
      "source": [
        "# 候補\n",
        "- refomerを行う => transfomerと比較\n",
        "- 同等の精度で計算量の違いを表す。\n",
        "\n",
        "## transfomerの問題点\n",
        "\n",
        "attentionの計算量が多い\n",
        "特に内積QK\n",
        "refomerではlocality-sensitive hashing(LSH)にを使う。\n",
        "\n",
        "## 注意\n",
        "実際のtransformerはMluti-Head-attentionだが今回はsimple transfomerを使う。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqOZceWcJN2x"
      },
      "source": [
        "## Transformerの構造\n",
        "1. 文章の単語ID列\n",
        "2. Embedderモジュール(word_id to word_vec)\n",
        "3. positionalEncoder\n",
        "4. TranformerBlock * 2\n",
        "    1. LayerNormalization\n",
        "    2. Attention(self-attention)\n",
        "    3. Dropout\n",
        "    4. Layer Normalization\n",
        "    5. FeedForwad\n",
        "    6. DropOut\n",
        "5. clasificationHead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ln_wZ7CUKb6"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N16iGoY38aEw"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    '''\n",
        "    word_id to vector_id\n",
        "    '''\n",
        "    def __init__(self, text_embedding_vectors):\n",
        "        super(Embeder, self).__init__()\n",
        "\n",
        "        # 日本語の学習済みを用意する\n",
        "        self.embeddings = nn.Embdding.from_pretraind(\n",
        "            embeddings = text_embedding_vectors, freeze = True\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x_vec = self.embeddings(x)\n",
        "        return x_vec"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD7f1ucfJB1z"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    '''\n",
        "    単語の位置を示すベクトル情報を付加する\n",
        "    '''\n",
        "    def __init__(self, vec_dim, max_seq_len):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "\n",
        "        self.vec_dim = vec_dim # 単語ベクトルの次元数\n",
        "        \n",
        "        pe = torch.zeros(max_seq_len, vec_dim)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, vec_dim, 2):\n",
        "                pe[pos, i]  = math.sin(pos/ (10000 ** ((2*i)/vec_dim)))\n",
        "                pe[pos, i + 1]  = math.cos(pos/ (10000 ** ((2*i)/vec_dim)))\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)  #　ミニバッチの次元を追加\n",
        "\n",
        "        self.pe.requires_grad = False #　勾配計算しない\n",
        "\n",
        "    def forward(self, x):\n",
        "        ret = math.sqrt(self.vec_dim)*x + self.pe\n",
        "        return ret"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZmti40BJb_Z"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model = 300):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1dconv(本来は)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        key = self.key(k)\n",
        "        query = self.key(q)\n",
        "        value = self.key(v)\n",
        "        \n",
        "        # 内積をとりqureyとkeyの関連度を計算する。\n",
        "        weights = torch.matmul(q, k.transpose(1,2))\n",
        "\n",
        "        # softmaxで0をとるため\n",
        "        mask = mask.unsqueeze(1)\n",
        "        weights = weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # softmaxで正規化(attention weight 0 ~ 1)\n",
        "        normlized_weights = F.softmax(weights, dim=-1)  # attention weight\n",
        "        output = torch.matmul(normlized_weights, value)\n",
        "\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, normlized_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH8n1PiEX6Vo"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout(F.relu(x))\n",
        "        return self.linear_2(x)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4h18j--blrn"
      },
      "source": [
        "class TransfomerBlock(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # layernormlize\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attn = Attention(d_model)\n",
        "        \n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x_normlized = self.norm_1(x)\n",
        "        output, normalized_weights = self.attn(\n",
        "            x_normlized, x_normlized, x_normlized, mask)\n",
        "        \n",
        "        x2 = x + self.dropout_1(output)\n",
        "\n",
        "        x_normlized2 = self.norm_2(x2)\n",
        "        \n",
        "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
        "\n",
        "        return output, normalized_weights  # [output, attention weights]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S7OqTsulU-p"
      },
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, d_model=300, out_dim = 2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # 重みを正規分布で初期化\n",
        "        nn.init.normal_(self.linear.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x[:, 0, :]  # [batch_size, text_len, depth]\n",
        "        out = self.linear(x0)\n",
        "        return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmQrCXuYnP4n"
      },
      "source": [
        "class TransformerClassification(nn.Module):\n",
        "    def __init__(self, text_embedding_vectors, d_mode=300, max_seq_len=256,out_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net1 = Embedder(text_embedding_vectors)\n",
        "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
        "        self.net3_1 = TransfomerBlock(d_model=d_model)\n",
        "        self.net3_2 = TransfomerBlock(d_model=d_model)\n",
        "        self.net4 = ClassificationHead(d_model=d_model, out_dim = out_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1 = self.net1(x)  # 単語をベクトルに\n",
        "        x2 = self.net2(x1)  # Positon情報を足し算\n",
        "        x3_1, normlized_weights_1 = self.net3_1(\n",
        "            x2, mask)  # Self-Attentionで特徴量を変換\n",
        "        x3_2, normlized_weights_2 = self.net3_2(\n",
        "            x3_1, mask)  # Self-Attentionで特徴量を変換\n",
        "        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
        "        return x4, normlized_weights_1, normlized_weights_2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myToCeYbUDHK"
      },
      "source": [
        "class ReAttention(Attention):\n",
        "    def __init__(self, d_model = 300):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        '''\n",
        "        locality-sensitive hashing (LSH)\n",
        "        計算式は不明、後日勉強\n",
        "        '''\n",
        "        pass"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vliB5YhnwjI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}